# Vertex AI-Powered Provider Search Chatbot

This project is a simple, yet powerful, web-based chatbot that leverages Google's Vertex AI to answer user queries. It is designed as a provider search tool, capable of understanding natural language questions to find relevant information and present it in a structured format.

A key feature of this application is its transparency. In "Provider2" mode, it not only provides the final answer but also shows the intermediate reasoning steps it took, including entity extraction (e.g., medical specialty, location) and the SQL query generated by the Large Language Model (LLM) to fetch the data.

The application is built with Flask and is designed to be deployed as a Google Cloud Function.

## Features

- **Interactive Web UI**: A clean, chat-style interface for user interaction, built with Flask and Jinja2.
- **Natural Language Understanding**: Utilizes Vertex AI's `text-bison` model to interpret user questions.
- **Transparent Reasoning**: The "Provider2" mode displays the LLM's reasoning process, showing:
  - Identified and verified specialties.
  - Identified location.
  - The exact SQL query generated to find the results.
- **Structured Output**: Displays final results in a clean, easy-to-read HTML table using the `tabulate` library.
- **Environment-based Configuration**: All settings, including GCP project details and model parameters, are configured via environment variables.
- **Cloud-Native**: Built using `functions-framework` for easy deployment to Google Cloud Functions.

## Architecture

1.  A user submits a question through the Flask web interface.
2.  The `main` function, acting as a Google Cloud Function entry point, receives the request.
3.  The application retrieves configuration from environment variables.
4.  Based on the user's selection ("Provider1" or "Provider2"), the corresponding backend logic is invoked.
5.  The backend logic constructs a prompt and sends it to the Vertex AI Text Generation Model (`text-bison`).
6.  The LLM processes the prompt to extract key entities (like provider specialty and location) and generates a corresponding SQL query.
7.  The application's backend logic (e.g., in `provider2.py`) would typically execute this query against a database to get the provider data.
8.  The final data is formatted into an HTML table.
9.  The Flask application renders the `ui.html` template, displaying the conversation history, the reasoning steps (for Provider2), and the final answer table.

## Setup and Installation

### Prerequisites

- Python 3.8+
- A Google Cloud Project with the **Vertex AI API** enabled.
- Authenticated `gcloud` CLI or Application Default Credentials (ADC) set up for your environment.

### 1. Clone the Repository

```bash
git clone <your-repository-url>
cd <repository-directory>
```

### 2. Create a Virtual Environment and Install Dependencies

It is highly recommended to use a virtual environment.

```bash
# Create a virtual environment
python3 -m venv venv
source venv/bin/activate

# Create a requirements.txt file with the following content:
```

**`requirements.txt`**
```txt
google-cloud-aiplatform
functions-framework
Flask
markdown
tabulate
```

```bash
# Install the dependencies
pip install -r requirements.txt
```

### 3. Configure Environment Variables

The application is configured using environment variables. You can create a `.env` file in the project root and set the variables there, or export them in your shell.

**Example `.env` file:**
```env
# GCP Configuration
project_id="your-gcp-project-id"
region="us-central1"

# UI Configuration
ui_title="Provider Search Bot"
ui_bot_name="ProviderBot"
ui_user_name="You"

# Vertex AI Model Configuration
llm_model="text-bison@002"
embedding_model="textembedding-gecko@001"
max_output_tokens=2048
temperature=0.2
top_p=0.8
top_k=40

# Deprecated - For former ChromaDB integration
chromadb_ip="127.0.0.1"
chromadb_port=8000
```

| Variable              | Description                                                                 | Default/Example         |
| --------------------- | --------------------------------------------------------------------------- | ----------------------- |
| `project_id`          | Your Google Cloud Project ID.                                               | `your-gcp-project-id`   |
| `region`              | The GCP region for Vertex AI services.                                      | `us-central1`           |
| `ui_title`            | The title displayed in the web UI.                                          | `Provider Search Bot`   |
| `ui_bot_name`         | The name for the chatbot in the UI.                                         | `ProviderBot`           |
| `ui_user_name`        | The name for the user in the UI.                                            | `You`                   |
| `llm_model`           | The Vertex AI text generation model to use.                                 | `text-bison@002`        |
| `embedding_model`     | The Vertex AI embedding model to use.                                       | `textembedding-gecko@001` |
| `max_output_tokens`   | The maximum number of tokens for the LLM to generate.                       | `2048`                  |
| `temperature`         | Controls the randomness of the LLM output (0.0 to 1.0).                     | `0.2`                   |
| `top_p`               | The cumulative probability of tokens to consider for sampling.              | `0.8`                   |
| `top_k`               | The number of highest probability tokens to consider for sampling.          | `40`                    |
| `chromadb_ip`         | (Deprecated) IP for ChromaDB. The related code is currently commented out.  | `127.0.0.1`             |
| `chromadb_port`       | (Deprecated) Port for ChromaDB. The related code is currently commented out.| `8000`                  |

## Running the Application

### Locally

The `functions-framework` allows you to run the application on a local development server that emulates the Google Cloud Functions environment.

```bash
# Make sure your environment variables are set (e.g., from your .env file)
# For example, if you don't have a .env file loader:
# export $(cat .env | xargs)

functions-framework --target=main --debug
```

The application will be available at `http://localhost:8080`.

### Deploying to Google Cloud Functions

This application is ready to be deployed as a 2nd generation Google Cloud Function.

```bash
gcloud functions deploy provider-search-bot \
  --gen2 \
  --runtime=python311 \
  --region=us-central1 \
  --source=. \
  --entry-point=main \
  --trigger-http \
  --allow-unauthenticated \
  --set-env-vars=project_id=your-gcp-project-id,region=us-central1,llm_model=text-bison@002 # ... and so on for other env vars
```

## Usage

1.  Navigate to the application's URL in your web browser.
2.  In the text box, type a question to find a provider. For example: `Find me a lung doctor in NYC`.
3.  Use the dropdown to select a "Question Type":
    -   **Provider1**: A direct answer mode.
    -   **Provider2**: The transparent mode that shows reasoning steps.
4.  Click "Submit" to see the response from the chatbot.

## Project Structure

```
├── main.py                   # Main Flask app, Cloud Function entry point
├── provider2.py              # Contains logic for the "Provider2" search
├── notebook.py               # Supporting module (likely from a notebook)
├── validate.py               # Supporting module for validation
├── templates/
│   └── ui.html               # Jinja2 template for the web interface
├── requirements.txt          # Python dependencies
└── README.md                 # This file
```

---